###############################################################################################
#                                      -General Options-                                      # 
###############################################################################################
# These parameters are required.                                                              #
###############################################################################################

# qcow2 image to use for your Virtual Machines: 
# Can be found here: https://cdimage.debian.org/cdimage/openstack/current/?C=M;O=A 
qcow2_image: 'https://cdimage.debian.org/cdimage/openstack/current/debian-10.0.2-20190721-openstack-amd64.qcow2'

# Location to use for storing the qcow2 image. Be sure to end the absolute path with a '/'.
qcow2_download_location: '/tmp/'

# Resource Pool for your virtual machines.
k8s_resource_pool: "Kubernetes"

# Path to the SSH Public Key on your Proxmox Server.
k8s_ssh_key: "/root/.ssh/sol.milkyway.kubernetes.pub"

# VM IDs
k8s_master_id: '4010'
k8s_node1_id: '40100'
k8s_node2_id: '40101'
k8s_node3_id: '40102'

# Hostnames
k8s_master_hn: "Pluto"
k8s_node1_hn: "Ceres"
k8s_node2_hn: "Eris"
k8s_node3_hn: "Haumea"

# Number of CPUs
k8s_master_cpu: "2"
k8s_node1_cpu: "4"
k8s_node2_cpu: "4"
k8s_node3_cpu: "4"

# Amount of memory expressed in megabytes
k8s_master_mem: "5120"
k8s_node1_mem: "10240"
k8s_node2_mem: "10240"
k8s_node3_mem: "10240"

# Disk Sizes
k8s_master_size: "50G"
k8s_node1_size: "50G"
k8s_node2_size: "50G"
k8s_node3_size: "50G"

# IP Addresses
k8s_master_ip: "192.168.40.10"
k8s_node1_ip: "192.168.40.100"
k8s_node2_ip: "192.168.40.101"
k8s_node3_ip: "192.168.40.102"

# Subnet Sizes
k8s_master_sn: "/24"
k8s_node1_sn: "/24"
k8s_node2_sn: "/24"
k8s_node3_sn: "/24"

# Network Gateway
k8s_master_gw: "192.168.40.1"
k8s_node1_gw: "192.168.40.1"
k8s_node2_gw: "192.168.40.1"
k8s_node3_gw: "192.168.40.1"

# DNS Servers
k8s_master_ns: "192.168.1.100"
k8s_node1_ns: "192.168.1.100"
k8s_node2_ns: "192.168.1.100"
k8s_node3_ns: "192.168.1.100"

# Search Domains
k8s_master_sd: "sol.milkyway"
k8s_node1_sd: "sol.milkyway"
k8s_node2_sd: "sol.milkyway"
k8s_node3_sd: "sol.milkyway"

# Network Bridges
k8s_master_bridge: "vmbr0"
k8s_node1_bridge: "vmbr0"
k8s_node2_bridge: "vmbr0"
k8s_node3_bridge: "vmbr0"

# Storage volumes
k8s_master_stg: "SaturnPool"
k8s_node1_stg: "SaturnPool"
k8s_node2_stg: "SaturnPool"
k8s_node3_stg: "SaturnPool"

# CIDR for the Calico Pod Network - MUST be different from your Kubernetes CIDR to avoid an IP conflict. Subnet size will automatically be set to /16.
calico_cidr: '172.16.0.0'

# URL for the Calico Network Policy: 
# Can be found here: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tabs-pod-install-2
calico_policy_url: 'https://docs.projectcalico.org/v3.8/manifests/calico.yaml'

# The URL for the Kubernetes Dashboard Manifest. Not locally hosted as it changes frequently.
# Can be found in the `Getting Started` section of this page: https://github.com/kubernetes/dashboard/blob/master/README.md
k8s_dasboard_url: 'https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml'

# Service user created for the dashboard.
k8s_user_name: 'tj'


###############################################################################################
#                                        -VLAN Options-                                       # 
###############################################################################################
# These parameters are only required if your network is configured with VLANs and you wish to #
# specify which VLAN each of your Kubernetes nodes is allocated to. Leave these variables     #
# blank if you do not intend to use VLANs for your cluster.                                   #
###############################################################################################

# VLAN Tags
k8s_master_vlan: "40"
k8s_node1_vlan: "40"
k8s_node2_vlan: "40"
k8s_node3_vlan: "40"


###############################################################################################
#                                        -NFS Options-                                        # 
###############################################################################################
# These parameters are only required if you have an NFS server and would like to enable the   #
# optional support for dynamic provisioning of Persistent Storage volumes for your pods.
###############################################################################################

# NFS server
nfs_hostname: 'saturn.sol.milkyway'

# NFS Mount Point
nfs_mount_point: '/SaturnPool/Kubernetes'

# NFS Provisioner Name
nfs_provisioner: 'saturnpool'

# Namespace to deploy the NFS Provisioner to.
nfs_namespace: 'nfs-provisioner'

# Default reclaim policy. https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy
nfs_reclaim_policy: 'Retain'


###############################################################################################
#                                      -MetalLB Options-                                      # 
###############################################################################################
#                                                                                             #
# WARNING: MetalLB is currently supported, however dynamic provisioning is NOT currently      #
# supported despite the explanation below. This will come in a later release, however, if you #
# are interested in using this feature you will need to modify the configmap located in the   #
# MetalLB files directory yourself to match the variables you declare here.                   #
#                                                                                             #
# These parameters are only required if you want to enable on-prem load balancing with        #
# MetalLB. The MetalLB configuration file will be dynamically provisioned based on how many   #
# variable sets you declare below. This is intended to allow you to provision a single or     #
# multiple address pools depending on your personal network configuration. The configuration  #
# is dynamically generated based on how many sets of variables you list below. For example:   #
#                                                                                             #
# If you wish to deploy a single address pool, you would declare these three variables.       #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
#                                                                                             #
#                                                                                             #
# However, if you wished to declare two address pools, you would include a second set of      #
# variables where the `ap1` portion of the variable name is incremeneted by one.              #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap2_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap2_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
# metallb_ap2_cidr: CIDR                                                                      #
###############################################################################################

# The IP Address of the router you wish to peer with.
metallb_peer_router: '192.168.1.1'

# The AS Number of the router you wish to peer with.
metallb_peer_asn: '64512'

# The names of your address pools.
metallb_ap1_name: 'VLAN20'
metallb_ap4_name: 'VLAN70'
metallb_ap2_name: 'VLAN50'
metallb_ap3_name: 'VLAN60'

# The protocols of your address pools. (bgp|layer2)
metallb_ap1_protocol: 'bgp'
metallb_ap2_protocol: 'bgp'
metallb_ap3_protocol: 'bgp'
metallb_ap4_protocol: 'bgp'

# The CIDRs of your address pools.
metallb_ap1_cidr: '192.168.20.1/24'
metallb_ap2_cidr: '192.168.50.1/24'
metallb_ap3_cidr: '192.168.60.1/24'
metallb_ap4_cidr: '192.168.70.1/24'


###############################################################################################
#                              -NGINX Ingress Controller Options-                             # 
###############################################################################################
# These parameters are only required if you want to enable the NGINX ingress controller.      #
# REQUIREMENT: The NGINX Ingress controller integration requires a Load Balancer integration. #
###############################################################################################

# The IP Address to use 
nginx_load_balancer_ip: '192.168.0.100'


###############################################################################################
#                                      -DataDog Options-                                      # 
###############################################################################################
# These parameters are only required if you have an account with DataDog and would like to    #
# enable metrics collection for your cluster.                                                 #
###############################################################################################

# Your DataDog API Key
dd_api_key: 'APIKEY'

# Namespace to deploy DataDog to
dd_namespace: 'default'
